{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This collection of scenarios demonstrates how to solve various data quality problems by exploiting patterns found (or validated) by Desbordante.\n",
        "\n",
        "In this scenario, we showcase a simple application that performs data deduplication in a table.\n",
        "\n",
        "The idea of this scenario is described in the paper \"Solving Data Quality Problems with Desbordante: a Demo\" by G. Chernishev et al., available at https://arxiv.org/abs/2307.14935. There is also an interactive demo at https://desbordante.streamlit.app/."
      ],
      "metadata": {
        "id": "ph-_mmDOGHje"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jF-5JlzE5e"
      },
      "source": [
        "# Data deduplication example using Desbordante algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ogzUWhlaWt",
        "outputId": "9bd12414-50d2-4e63-9c30-c4fde3111ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting desbordante==2.3.2\n",
            "  Downloading desbordante-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Downloading desbordante-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: desbordante\n",
            "Successfully installed desbordante-2.3.2\n",
            "--2025-03-20 18:05:28--  https://raw.githubusercontent.com/Desbordante/desbordante-core/refs/heads/main/examples/datasets/duplicates.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4466 (4.4K) [text/plain]\n",
            "Saving to: ‘duplicates.csv’\n",
            "\n",
            "duplicates.csv      100%[===================>]   4.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-20 18:05:29 (42.6 MB/s) - ‘duplicates.csv’ saved [4466/4466]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install desbordante==2.3.2\n",
        "!wget https://raw.githubusercontent.com/Desbordante/desbordante-core/refs/heads/main/examples/datasets/duplicates.csv\n",
        "\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import desbordante\n",
        "import pandas\n",
        "\n",
        "\n",
        "def setup_pandas_print():\n",
        "    pandas.set_option('display.max_columns', None)\n",
        "    pandas.set_option('display.width', None)\n",
        "    pandas.set_option('display.max_colwidth', None)\n",
        "    pandas.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "setup_pandas_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRAc6mNW5T6_"
      },
      "source": [
        "## Setting up various algorithm parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nty20bIy0i2W"
      },
      "outputs": [],
      "source": [
        "# Algorithm that finds approximate FDs and its config\n",
        "ALGORITHM_TYPE = desbordante.afd.algorithms.Default\n",
        "ERROR = 0.001\n",
        "CONFIG = {'error': ERROR, 'max_lhs': 1}\n",
        "\n",
        "# Parameters for pandas.read_csv(...).\n",
        "DATASET_PATH = 'duplicates.csv'\n",
        "HEADER = 0\n",
        "SEPARATOR = ','\n",
        "\n",
        "# File where the deduplicated dataset will be written.\n",
        "OUTPUT_FILE = 'output.csv'\n",
        "\n",
        "# Initial window size in sorted neighborhood method.\n",
        "INITIAL_WINDOW_SIZE = 4\n",
        "\n",
        "# Variable to simplify the configuration string construction below.\n",
        "ALGORITHM = ALGORITHM_TYPE.__name__\n",
        "\n",
        "# A message containing all variables used by this usage scenario, to be\n",
        "# displayed to the user.\n",
        "CONFIG_STRING = f\"\"\"Deduplication parameters:\n",
        "{ALGORITHM=}\n",
        "{ERROR=:.5f}\n",
        "{DATASET_PATH=}\n",
        "{SEPARATOR=}\n",
        "{INITIAL_WINDOW_SIZE=}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDJ7mdTx5uoh"
      },
      "source": [
        "## Defining necessary functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBa3kfddu4cZ"
      },
      "outputs": [],
      "source": [
        "def get_1lhs_fds(df, algo_type, algo_config):\n",
        "    algo = algo_type()\n",
        "    algo.load_data(table=df, **algo_config)\n",
        "    algo.execute(**algo_config)\n",
        "    return sorted((lhs_indices[0], fd.rhs_index) for fd in algo.get_fds()\n",
        "                  if len(lhs_indices := fd.lhs_indices) == 1)\n",
        "\n",
        "\n",
        "def get_lhs_from_sorted_fds(fds):\n",
        "    lhs = []\n",
        "    prev_lhs = None\n",
        "    for cur_lhs, _ in fds:\n",
        "        if cur_lhs != prev_lhs:\n",
        "            lhs.append(cur_lhs)\n",
        "        prev_lhs = cur_lhs\n",
        "    return lhs\n",
        "\n",
        "\n",
        "def count_matches(row1, row2, rhs: list[int]):\n",
        "    return sum(row1[index] == row2[index] for index in rhs)\n",
        "\n",
        "\n",
        "def print_fd_info(df: pandas.DataFrame, fds: list[tuple[int, int]]):\n",
        "    fd_dict = defaultdict(list)\n",
        "    for lhs, rhs in fds:\n",
        "        fd_dict[lhs].append(df.columns[rhs])\n",
        "    print('AFD info:')\n",
        "    print('\\n'.join(f'{lhs}: {df.columns[lhs]} -> ( {\" \".join(fd_dict[lhs])} )'\n",
        "                    for lhs in get_lhs_from_sorted_fds(fds)))\n",
        "\n",
        "\n",
        "def keepall_handler(df, new_rows, remaining_rows, used_rows):\n",
        "    new_rows.extend(df.iloc[list(remaining_rows)].itertuples(index=False))\n",
        "    remaining_rows.clear()\n",
        "\n",
        "\n",
        "def drop_handler(df, new_rows, remaining_rows, used_rows):\n",
        "    indices_to_add = list(remaining_rows - used_rows)\n",
        "    new_rows.extend(df.iloc[indices_to_add].itertuples(index=False))\n",
        "    remaining_rows.clear()\n",
        "\n",
        "\n",
        "def choose_index(col_name, distinct_values):\n",
        "    print(f'Column: {col_name}. Which value to use?')\n",
        "    print('\\n'.join(f'{i}: {value}' for i, value in enumerate(distinct_values)))\n",
        "    return int(input('index: '))\n",
        "\n",
        "\n",
        "def merge_handler(df: pandas.DataFrame, new_rows, remaining_rows, used_rows):\n",
        "    if not used_rows:\n",
        "        return\n",
        "    new_row = []\n",
        "    for col_name, values in zip(df.columns,\n",
        "                                zip(*df.iloc[list(used_rows)].itertuples(index=False))):\n",
        "        distinct_values = list(set(values))\n",
        "        index = 0 if len(distinct_values) == 1 else choose_index(col_name, distinct_values)\n",
        "        new_row.append(distinct_values[index])\n",
        "    remaining_rows -= used_rows\n",
        "    new_rows.append(new_row)\n",
        "\n",
        "\n",
        "def unknown_handler(df, new_rows, remaining_rows, used_rows):\n",
        "    print('Unknown command.')\n",
        "\n",
        "\n",
        "def ask_rows(df: pandas.DataFrame, window: deque[tuple[int, object]]) -> list:\n",
        "    commands = {\n",
        "        'keepall': keepall_handler,\n",
        "        'drop': drop_handler,\n",
        "        'merge': merge_handler,\n",
        "    }\n",
        "\n",
        "    remaining_rows = {row_info[0] for row_info in window}\n",
        "    new_rows = []\n",
        "    while remaining_rows:\n",
        "        print(df.iloc[sorted(remaining_rows)].to_string())\n",
        "        command_args = input('Command: ').split()\n",
        "        if not command_args:\n",
        "            print('Please input a command!')\n",
        "            continue\n",
        "        command, *used_rows = command_args\n",
        "        used_rows = {col_num for col in used_rows if (col_num := int(col)) in remaining_rows}\n",
        "        commands.get(command, unknown_handler)(df, new_rows, remaining_rows, used_rows)\n",
        "    return new_rows\n",
        "\n",
        "\n",
        "def is_similar(row_info, window, chosen_cols, matches_required):\n",
        "    return any(count_matches(prev_row_info[1], row_info[1], chosen_cols) >= matches_required\n",
        "               for prev_row_info in window)\n",
        "\n",
        "\n",
        "def get_deduped_rows(df: pandas.DataFrame, chosen_cols: list[int], matches_required: int,\n",
        "                     fds: list[tuple[int, int]]):\n",
        "    df.sort_values([df.columns[rhs_col] for _, rhs_col in fds if rhs_col in chosen_cols],\n",
        "                   inplace=True)\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    window = deque()\n",
        "    new_rows = []\n",
        "    has_duplicate = False\n",
        "    for row_info in df.iterrows():\n",
        "        if len(window) < INITIAL_WINDOW_SIZE:\n",
        "            if not has_duplicate:\n",
        "                has_duplicate = is_similar(row_info, window, chosen_cols, matches_required)\n",
        "        elif not has_duplicate:\n",
        "            new_rows.append(window.pop()[1].values)\n",
        "            has_duplicate = is_similar(row_info, window, chosen_cols, matches_required)\n",
        "        elif not is_similar(row_info, window, chosen_cols, matches_required):\n",
        "            new_rows.extend(ask_rows(df, window))\n",
        "            window.clear()\n",
        "            has_duplicate = False\n",
        "        window.appendleft(row_info)\n",
        "    new_rows.extend(\n",
        "        ask_rows(df, window) if has_duplicate else (row_info[1].values for row_info in window))\n",
        "    return new_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfxlnUdf3FV5"
      },
      "source": [
        "## Printing dataset sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdMljz5E15pX",
        "outputId": "15aaf14c-3ec2-415c-f3f2-d2e8d7be7e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id             name address       city                          email phone country\n",
            "0   5996        Kaede Sue      66      Pirus       Kaede.Sue4422@virtex.rum    39      EU\n",
            "1     36       Licia Wolf      35  Pilington       Licia.Wolf1260@cmail.com    35      CM\n",
            "2     17        Steve Doe      16     Syndye           Steve.Doe272@muli.ry    16      GZ\n",
            "3     62      Lisa Tarski      61     Syndye     Lisa.Tarski3782@virtex.rum    61      JU\n",
            "4      6      Mary Tarski       5     Lumdum       Mary.Tarski30@ferser.edu     5      PR\n",
            "..   ...              ...     ...        ...                            ...   ...     ...\n",
            "73    15        Ivan Dawn      14     Syndye      Ivan.Dawn210@atomlema.ocg    14      FC\n",
            "74  5993       Lisa Honjo      63       Roit      Lisa.Honjo4032@virtex.rum    63      AI\n",
            "75    59         Lisa Sue      58     Muxicu         Lisa.Sue3422@cmail.com    58      AI\n",
            "76    21  Steve Shiramine      20  Pilington  Steve.Shiramine420@ferser.edu    20      GZ\n",
            "77    44      Maxine Wolf      43     Muxicu   Maxine.Wolf1892@atomlema.ocg    43      PR\n",
            "\n",
            "[78 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pandas.read_csv(DATASET_PATH, sep=SEPARATOR, header=HEADER, dtype=str, index_col=False)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XtQJov86Lgo"
      },
      "source": [
        "## Starting deduplication scenario with parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLvN6CRF6Pcf",
        "outputId": "a487b64d-053c-4144-955e-81c26e12015a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduplication parameters:\n",
            "ALGORITHM='Pyro'\n",
            "ERROR=0.00100\n",
            "DATASET_PATH='duplicates.csv'\n",
            "SEPARATOR=','\n",
            "INITIAL_WINDOW_SIZE=4\n"
          ]
        }
      ],
      "source": [
        "print(CONFIG_STRING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SXtg3-h6K_0",
        "outputId": "7b398a34-631d-4001-d66e-a8f87fef44e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original records: 78\n",
            "\n",
            "AFD info:\n",
            "0: id -> ( name address city email phone country )\n",
            "2: address -> ( name )\n",
            "4: email -> ( name address phone country )\n",
            "5: phone -> ( name )\n",
            "LHS column index: 2\n",
            "RHS columns:\n",
            "1: name\n",
            "RHS columns to use (indices): 1\n",
            "Equal columns to consider duplicates: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fdfd4ffba5f1>:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return sum(row1[index] == row2[index] for index in rhs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id          name address      city                       email phone country\n",
            "5     27     Björn Sue      26      Roit      Björn.Sue702@cmail.com    26      CM\n",
            "6     30  Björn Tarski      29    Lumdum  Björn.Tarski870@ferser.edu    29      PR\n",
            "7  11886    Björn Wolf      28  Kustruma    Björn.Wolf756@virtex.rum    27      AI\n",
            "8   5957    Björn Wolf      27       NaN    Björn.Wolf756@virtex.rum    27      AI\n",
            "9     28    Björn Wolf      27       NaN    Björn.Wolf756@virtex.rum    27      AI\n",
            "Command: drop 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fdfd4ffba5f1>:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return sum(row1[index] == row2[index] for index in rhs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id        name address       city                       email phone country\n",
            "42    63   Lisa Dawn      62  Pilington  Lisa.Dawn3906@atomlema.ocg    62      EU\n",
            "43    57    Lisa Doe      56       Roit     Lisa.Doe3192@virtex.rum    56      AI\n",
            "44  5993  Lisa Honjo      63       Roit   Lisa.Honjo4032@virtex.rum    63      AI\n",
            "45    64  Lisa Honjo      63      Pirus   Lisa.Honjo4032@virtex.rum    63      AI\n",
            "Command: keepall\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fdfd4ffba5f1>:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return sum(row1[index] == row2[index] for index in rhs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id       name address      city                        email phone country\n",
            "50     60  Lisa Wolf      59    Syndye      Lisa.Wolf3540@cmail.com    59      FC\n",
            "51      7  Mary Dawn       6    Syndye     Mary.Dawn42@atomlema.ocg     6      PR\n",
            "52  11859   Mary Doe     NaN    Lumdum     Mary.Doe-5926@ferser.edu     0      EU\n",
            "53      1   Mary Doe     NaN    Lumdum            Mary.Doe0@muli.ry     4      EU\n",
            "54  17788   Mary Doe       0  Kustruma  Mary.Doe35099692@virtex.rum     0      EU\n",
            "55   5930   Mary Doe     NaN    Lumdum     Mary.Doe-5926@ferser.edu     0     NaN\n",
            "Command: merge 52 55\n",
            "Column: id. Which value to use?\n",
            "0: 11859\n",
            "1: 5930\n",
            "index: 1\n",
            "Column: country. Which value to use?\n",
            "0: nan\n",
            "1: EU\n",
            "index: 1\n",
            "       id       name address      city                        email phone country\n",
            "50     60  Lisa Wolf      59    Syndye      Lisa.Wolf3540@cmail.com    59      FC\n",
            "51      7  Mary Dawn       6    Syndye     Mary.Dawn42@atomlema.ocg     6      PR\n",
            "53      1   Mary Doe     NaN    Lumdum            Mary.Doe0@muli.ry     4      EU\n",
            "54  17788   Mary Doe       0  Kustruma  Mary.Doe35099692@virtex.rum     0      EU\n",
            "Command: keepall\n",
            "\n",
            "Resulting records: 76. Duplicates found: 2\n",
            "    id             name address       city                          email phone country\n",
            "0   31       Björn Dawn      30     Muxicu     Björn.Dawn930@atomlema.ocg    30      JU\n",
            "1   25        Björn Doe      24  Pilington           Björn.Doe600@muli.ry    24      FC\n",
            "2   32      Björn Honjo      31   Kustruma      Björn.Honjo992@virtex.rum    31      RI\n",
            "3   29  Björn Shiramine      28     Syndye  Björn.Shiramine812@virtex.rum    28      EU\n",
            "4   26      Björn Smith      25  Pilington      Björn.Smith650@virtex.rum    25      RI\n",
            "..  ..              ...     ...        ...                            ...   ...     ...\n",
            "71  21  Steve Shiramine      20  Pilington  Steve.Shiramine420@ferser.edu    20      GZ\n",
            "72  20       Steve Wolf      19  Pilington          Steve.Wolf380@muli.ry    19      RI\n",
            "73  22     Steve Tarski      21  Pilington   Steve.Tarski462@atomlema.ocg    21      PR\n",
            "74  19        Steve Sue      18     Syndye        Steve.Sue342@virtex.rum    18      AI\n",
            "75  18      Steve Smith      17     Lumdum       Steve.Smith306@cmail.com    17      EU\n",
            "\n",
            "[76 rows x 7 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fdfd4ffba5f1>:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return sum(row1[index] == row2[index] for index in rhs)\n"
          ]
        }
      ],
      "source": [
        "print(f'Original records: {len(df)}')\n",
        "print()\n",
        "\n",
        "fds = get_1lhs_fds(df, ALGORITHM_TYPE, CONFIG)\n",
        "print_fd_info(df, fds)\n",
        "lhs_column = int(input('LHS column index: '))\n",
        "fds = list(filter(lambda fd: fd[0] == lhs_column, fds))\n",
        "if not fds:\n",
        "    print('No FDs with this LHS!')\n",
        "else:\n",
        "    print('RHS columns:')\n",
        "    print('\\n'.join(f'{rhs}: {df.columns[rhs]}' for _, rhs in fds))\n",
        "    chosen_cols = sorted(set(map(int, input('RHS columns to use (indices): ').split())))\n",
        "    matches_required = int(input('Equal columns to consider duplicates: '))\n",
        "\n",
        "    new_rows = get_deduped_rows(df, chosen_cols, matches_required, fds)\n",
        "    print()\n",
        "\n",
        "    print(f'Resulting records: {len(new_rows)}. Duplicates found: {len(df) - len(new_rows)}')\n",
        "    new_df = pandas.DataFrame(new_rows, columns=df.columns)\n",
        "\n",
        "    print(new_df)\n",
        "    new_df.to_csv(OUTPUT_FILE, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}